import os
import re
import webbrowser
from PIL import Image
from Agent_Marketplace.utils import smart_resize, encode_image, draw_point
import time

def make_screenshot(page, output_image_path):
    time.sleep(3)
    page.screenshot(path=output_image_path)
    return page


def open_browser(p):
    browser = p.chromium.launch(headless=False)
    # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    page = browser.new_page()
    # –ø–µ—Ä–µ—Ö–æ–¥ –ø–æ url –∞–¥—Ä–µ—Å—É:
    page.goto('https://market.yandex.ru/')
    return browser, page


def click(
        page,
        screenshot_path,
        user_query,
        client_openai,
        model_id,
        output_image_path="screenshot_annotated2.png",
        min_pixels=3136,
        max_pixels=12845056,
        pretty_click = False

):
    """Tool –∫–ª–∏–∫–∞ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç [x, y]"""

    input_image = Image.open(screenshot_path)

    # --- Smart resize: –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è Qwen3-VL ---
    resized_h, resized_w = smart_resize(
        input_image.height,
        input_image.width,
        factor=32,
        min_pixels=min_pixels,
        max_pixels=max_pixels,
    )
    resized_image = input_image.resize((resized_w, resized_h))

    base64_image = encode_image(screenshot_path)

    # –°–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{base64_image}"},
                },
                {"type": "text", "text": user_query},
            ],
        }
    ]

    # –ó–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏
    completion = client_openai.chat.completions.create(
        model=model_id,
        messages=messages,
        max_tokens=500,
    )

    output_text = completion.choices[0].message.content
    print("\n=== Model Output ===")
    print(output_text)

    # =================================================================
    #   –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ö–û–û–†–î–ò–ù–ê–¢ (Qwen —Ñ–æ—Ä–º–∞—Ç ‚Äî –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ 0‚Äì1000)
    # =================================================================

    coordinate_absolute = None

    # Bounding-box
    bbox = re.search(r'\[([0-9]+),\s*([0-9]+),\s*([0-9]+),\s*([0-9]+)\]', output_text)
    if bbox:
        x1, y1, x2, y2 = map(int, bbox.groups())
        rel_x = (x1 + x2) // 2
        rel_y = (y1 + y2) // 2
        print(f"Extracted bbox ‚Üí center = ({rel_x}, {rel_y})")

    # –¢–æ–ª—å–∫–æ —Ç–æ—á–∫–∞
    else:
        pt = re.search(r'\[([0-9]+),\s*([0-9]+)\]', output_text)
        if pt:
            rel_x, rel_y = map(int, pt.groups())
            print(f"Extracted point = ({rel_x}, {rel_y})")
        else:
            print(" Could not parse point or bbox from model output")
            return output_text, None

    # --- –ü–µ—Ä–µ–≤–æ–¥ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (0‚Äì1000) ‚Üí –ø–∏–∫—Å–µ–ª–∏ resized –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è ---
    abs_x = rel_x / 1000 * resized_w
    abs_y = rel_y / 1000 * resized_h
    coordinate_absolute = [abs_x, abs_y]

    # =================================================================
    #       –†–ò–°–£–ï–ú –¢–û–ß–ö–£ –ù–ê RESIZED –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ò
    # =================================================================

    annotated = draw_point(resized_image, coordinate_absolute, color="green")
    if pretty_click:
        annotated.save(output_image_path, quality=95)

        print(f"\nAnnotated image saved to: {os.path.abspath(output_image_path)}")

    if os.path.exists(output_image_path):
        webbrowser.open(f"file://{os.path.abspath(output_image_path)}")

    # --- –ü–µ—Ä–µ–≤–æ–¥ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (0‚Äì1000) ‚Üí –ø–∏–∫—Å–µ–ª–∏ –û–†–ò–ì–ò–ù–ê–õ–¨–ù–û–ì–û viewport'–∞ ---
    x_on_page = rel_x / 1000 * input_image.width
    y_on_page = rel_y / 1000 * input_image.height

    print(f"Clicking at ({x_on_page:.1f}, {y_on_page:.1f}) on page (viewport px)")

    page.mouse.click(x_on_page, y_on_page)
    return page

def click_and_type(
    page,
    screenshot_path,
    user_query,
    client_openai,
    model_id,
    text_to_type,
    output_image_path="screenshot_annotated_input.png",
    min_pixels=3136,
    max_pixels=12845056,
    pretty_click=False,
    press_enter=True,
):
    """
    Tool: –Ω–∞—Ö–æ–¥–∏—Ç —ç–ª–µ–º–µ–Ω—Ç –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—Ç—Ä–æ–∫—É –ø–æ–∏—Å–∫–∞), –∫–ª–∏–∫–∞–µ—Ç –≤ –Ω–µ–≥–æ –∏ –≤–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç.

    Args:
        page: Playwright page object
        screenshot_path: –ø—É—Ç—å –∫ —Å–∫—Ä–∏–Ω—à–æ—Ç—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        user_query: –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ù–∞–π–¥–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å—Ç—Ä–æ–∫–∏ –ø–æ–∏—Å–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [x, y]")
        client_openai: OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∫–ª–∏–µ–Ω—Ç
        model_id: ID –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "Qwen/Qwen3-VL-...")
        text_to_type: —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –≤–≤–µ—Å—Ç–∏ –ø–æ—Å–ª–µ –∫–ª–∏–∫–∞
        output_image_path: –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∏–Ω—à–æ—Ç
        min_pixels / max_pixels: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã smart_resize (–¥–ª—è Qwen-VL)
        pretty_click: —Ä–∏—Å–æ–≤–∞—Ç—å –ª–∏ —Ç–æ—á–∫—É –∏ –æ—Ç–∫—Ä—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        press_enter: –Ω–∞–∂–∏–º–∞—Ç—å –ª–∏ Enter –ø–æ—Å–ª–µ –≤–≤–æ–¥–∞

    Returns:
        page: –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π page object
    """

    input_image = Image.open(screenshot_path)

    # --- Smart resize: –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è Qwen3-VL ---
    resized_h, resized_w = smart_resize(
        input_image.height,
        input_image.width,
        factor=32,
        min_pixels=min_pixels,
        max_pixels=max_pixels,
    )
    resized_image = input_image.resize((resized_w, resized_h))

    base64_image = encode_image(screenshot_path)

    # –°–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{base64_image}"},
                },
                {"type": "text", "text": user_query},
            ],
        }
    ]

    # –ó–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏
    completion = client_openai.chat.completions.create(
        model=model_id,
        messages=messages,
        max_tokens=500,
    )

    output_text = completion.choices[0].message.content
    print("\n=== Model Output (click_and_type) ===")
    print(output_text)

    # =================================================================
    #   –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ö–û–û–†–î–ò–ù–ê–¢ (Qwen —Ñ–æ—Ä–º–∞—Ç ‚Äî –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ 0‚Äì1000)
    # =================================================================

    rel_x = rel_y = None

    # Bounding-box
    bbox = re.search(r'\[([0-9]+),\s*([0-9]+),\s*([0-9]+),\s*([0-9]+)\]', output_text)
    if bbox:
        x1, y1, x2, y2 = map(int, bbox.groups())
        rel_x = (x1 + x2) // 2
        rel_y = (y1 + y2) // 2
        print(f"Extracted bbox ‚Üí center = ({rel_x}, {rel_y})")

    # –¢–æ–ª—å–∫–æ —Ç–æ—á–∫–∞
    else:
        pt = re.search(r'\[([0-9]+),\s*([0-9]+)\]', output_text)
        if pt:
            rel_x, rel_y = map(int, pt.groups())
            print(f"Extracted point = ({rel_x}, {rel_y})")
        else:
            print("‚ùå Could not parse point or bbox from model output")
            return page  # or raise ValueError

    # --- –ü–µ—Ä–µ–≤–æ–¥ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (0‚Äì1000) ‚Üí –ø–∏–∫—Å–µ–ª–∏ viewport'–∞ (–æ—Ä–∏–≥–∏–Ω–∞–ª–∞) ---
    x_on_page = rel_x / 1000 * input_image.width
    y_on_page = rel_y / 1000 * input_image.height

    print(f"üñ±Ô∏è  Clicking at ({x_on_page:.1f}, {y_on_page:.1f}) on page (viewport {input_image.width}√ó{input_image.height})")

    # --- –ö–ª–∏–∫ ---
    page.mouse.click(x_on_page, y_on_page)
    time.sleep(0.3)  # –¥–∞—Ç—å —Ñ–æ–∫—É—Å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è

    # --- –í–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ ---
    print(f"‚å®Ô∏è  Typing: '{text_to_type}'")
    page.keyboard.type(text_to_type, delay=50)  # delay –∏–º–∏—Ç–∏—Ä—É–µ—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫—É—é —Å–∫–æ—Ä–æ—Å—Ç—å

    # --- –ù–∞–∂–∞—Ç–∏–µ Enter (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) ---
    if press_enter:
        print("‚èé  Pressing Enter")
        page.keyboard.press("Enter")

    # =================================================================
    #       –†–ò–°–£–ï–ú –¢–û–ß–ö–£ –ù–ê RESIZED –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ò (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
    # =================================================================

    # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ ‚Äî –Ω–∞ resized –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
    abs_x_annot = rel_x / 1000 * resized_w
    abs_y_annot = rel_y / 1000 * resized_h

    annotated = draw_point(resized_image, [abs_x_annot, abs_y_annot], color="blue")
    if pretty_click:
        annotated.save(output_image_path, quality=95)
        print(f"\nüñºÔ∏è  Annotated image saved to: {os.path.abspath(output_image_path)}")
        webbrowser.open(f"file://{os.path.abspath(output_image_path)}")

    return page
def scroll(
    page,
    direction="down",
    amount=300,
    smooth=True,
    delay_after=0.8
):
    if direction not in ("up", "down"):
        raise ValueError("direction must be 'up' or 'down'")

    scroll_delta = -amount if direction == "up" else amount

    current_scroll_y = page.evaluate("window.scrollY")
    new_scroll_y = max(0, current_scroll_y + scroll_delta)

    print(f"üñ±Ô∏è  Scroll {direction} by {amount}px ‚Üí from Y={current_scroll_y:.0f} to Y={new_scroll_y:.0f}")

    if smooth:
        # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û: –ø–µ—Ä–µ–¥–∞—ë–º —Ñ—É–Ω–∫—Ü–∏—é + –∞—Ä–≥—É–º–µ–Ω—Ç
        page.evaluate(
            """(y) => {
                window.scrollTo({
                    top: y,
                    behavior: 'smooth'
                });
            }""",
            new_scroll_y
        )
    else:
        # ‚úÖ –¢–∞–∫–∂–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
        page.evaluate("window.scrollTo(0, arguments[0])", new_scroll_y)

    time.sleep(delay_after)
    return page


def click_card_and_return_image_url_if_match(page, product_title):
    # –û—á–∏—Å—Ç–∏–º –∏ —É–ø—Ä–æ—Å—Ç–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
    import re
    clean_title = re.sub(r'[^\w\s]', ' ', product_title.lower())
    words = clean_title.split()

    # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –≤ alt –∫–æ—Ç–æ—Ä–æ–≥–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã 2 —Å–ª–æ–≤–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
    js_code = f"""
    () => {{
        const words = {words};
        const images = Array.from(document.querySelectorAll('img[alt]'));
        for (const img of images) {{
            const alt = img.alt.toLowerCase();
            let matchCount = 0;
            for (const word of words) {{
                if (alt.includes(word)) matchCount++;
            }}
            if (matchCount >= Math.min(2, words.length)) {{
                return img.src || img.dataset.src || null;
            }}
        }}
        return null;
    }}
    """
    try:
        return page.evaluate(js_code)
    except:
        return None

def describe_product_from_image(
        screenshot_path,
        user_query,
        client_openai,
        model_id,
        output_image_path="screenshot_annotated2.png",
        min_pixels=3136,
        max_pixels=12845056,
):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–±–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç)"""

    input_image = Image.open(screenshot_path)

    # --- Smart resize: –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è Qwen3-VL ---
    resized_h, resized_w = smart_resize(
        input_image.height,
        input_image.width,
        factor=32,
        min_pixels=min_pixels,
        max_pixels=max_pixels,
    )
    resized_image = input_image.resize((resized_w, resized_h))

    base64_image = encode_image(screenshot_path)

    if not base64_image:
        raise ValueError("Failed to encode image")

    # –°–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{base64_image}"},
                },
                {"type": "text", "text": user_query},
            ],
        }
    ]

    # –ó–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏
    completion = client_openai.chat.completions.create(
        model=model_id,
        messages=messages,
        max_tokens=500,
    )

    output_text = completion.choices[0].message.content
    print("\n=== Model Output ===")
    print(output_text)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç ‚Äî –±–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∏ —Ä–∏—Å–æ–≤–∞–Ω–∏—è —Ç–æ—á–µ–∫
    return output_text, output_image_path